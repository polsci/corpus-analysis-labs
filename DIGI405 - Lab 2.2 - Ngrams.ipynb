{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2732d05",
   "metadata": {},
   "source": [
    "# DIGI405 Lab 2.2: Clusters/Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac533c9",
   "metadata": {},
   "source": [
    "## Lab 2 Introduction\n",
    "\n",
    "In the lab notebooks for module 2 we introduce collocation analysis, analysis of clusters and n-grams, and\n",
    "keyword analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fd349",
   "metadata": {},
   "source": [
    "### A note about the Quake Stories v2 corpus\n",
    "\n",
    "This notebook works with the Quake Stories v2 (QSv2) corpus. This data comes from\n",
    "http://www.quakestories.govt.nz/, and consists of crowd-sourced accounts of earthquake experiences\n",
    "following the 2011 Canterbury earthquakes. This corpus contains 487 self-reported stories of\n",
    "earthquake experiences from 2011 to 2019. It is licensed under Creative Commons BY-NC-SA. Please\n",
    "be aware that some stories may relate to people who were killed or injured in the earthquakes. Please\n",
    "treat the material with respect.\n",
    "\n",
    "Remember, you can read about the filename format in the README file included in the corpus zip\n",
    "file. This provides a way for you to view the original web page that each text was scraped from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b87518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conc.corpus import Corpus\n",
    "from conc.conc import Conc\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = f'/srv/source-data/' # path to the source data from which the corpus will be created\n",
    "save_path = f'/srv/corpora/' # path to the directory where corpora are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus().load(f'{save_path}quake-stories-v2.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a85752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are running the code on your own machine, unzip the source files - adjust the corpus_source_path below to point to the directory with the source files\n",
    "# # uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# corpus_source_path = f'{source_path}quake-stories-v2.zip'\n",
    "# try:\n",
    "#     corpus = Corpus().load(f'{save_path}quake-stories-v2.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     corpus = Corpus(name = 'Quake Stories v2', description = 'This is a corpus based on stories from the http://www.quakestories.govt.nz/ website established by ManatÅ« Taonga / Ministry for Culture and Heritage in 2011. QuakeStories was a place for the public to share stories of these and subsequent New Zealand earthquakes. The site was licensed under Creative Commons BY-NC-SA (https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en). This data-set, as a re-representation of the stories, is also released under BY-NC-SA. Please be aware that some stories may relate to people who were killed or injured in the Canterbury earthquakes. Treat the material with respect. ').build_from_files(corpus_source_path, f'{save_path}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38939ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.summary() # overview of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = Conc(corpus) #initialize Conc reporting with the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92277cc0",
   "metadata": {},
   "source": [
    "If you want to examine the contents of the source data for the Quake Stories v2 corpus, uncomment the following cell and run it. This will copy it into your current working directory. You can download it to your local computer by right clicking on the file in file viewer and clicking 'Download'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae47b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = '/srv/source-data/quake-stories-v2.zip' # path to the file \n",
    "# destination = os.path.join(os.getcwd(), os.path.basename(source_file))\n",
    "# shutil.copy(source_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913f3a3",
   "metadata": {},
   "source": [
    "## Clusters / N-grams\n",
    "\n",
    "The lab notebooks on collocation analysis (2.1) and concordancing (1.2) introduce ways to \n",
    "investigate collocation patterns in corpora. The course material this week introduced multi-word \n",
    "expressions (MWEs) as one example of a collocation pattern. To study collocation patterns related to \n",
    "consecutive tokens in a corpus, we can examine the frequency of n-grams. N-grams are sequences of\n",
    "linguistic units that occur in a corpus. These are often tokens, but could be sequences of characters \n",
    "or other sub-token units. In this notebook, we will focus on token n-grams. \n",
    "\n",
    "The \"N\" in n-gram refers to the number of tokens in the sequence.  A single token is a unigram or 1-gram, \n",
    "two consecutive tokens are a bigram or 2-gram, three \n",
    "consecutive tokens are a trigram or 3-gram, and beyond this it is common to refer to n-grams using \n",
    "the number of tokens (e.g. 4-gram).  \n",
    "\n",
    "For the phrase:  \n",
    "> \"I do not like to eat marmite\"  \n",
    "\n",
    "bigrams would be:  \n",
    "> \"I do\", \"do not\", \"not like\", \"like to\", \"to eat\", \"eat marmite\"  \n",
    "\n",
    "and trigrams would be:\n",
    "> \"I do not\", \"do not like\", \"not like to\", \"like to eat\", \"to eat marmite\"  \n",
    "\n",
    "N-grams have relevance for many machine learning tasks. N-grams can be used to represent text \n",
    "in a way that preserves some of the meaning encoded in the order of tokens. For example, if we \n",
    "represent a document using bigrams, we can quantify instances of \"not like\" and instances of \"like\" without \n",
    "negation to get an indication of sentiment expressed in the text. Very basic statistical language \n",
    "models, like those used for predictive text, can be developed using n-grams. An n-gram based model predicts \n",
    "the next token by finding the most frequent n-gram that begins with the already given n-1 tokens \n",
    "(e.g. given \"have a nice\", such a model might predict \"have a nice day\" rather than \"have a nice funeral\").\n",
    "\n",
    "In corpus linguistics software and literature, it is common to refer to n-grams containing \n",
    "a specific token or token sequence as \"clusters\". This should not be confused with machine learning \n",
    "clustering techniques. \n",
    "\n",
    "This lab notebook will introduce you to ngram analysis. This notebook will continue analysis of the Quake Stories v2 corpus introduced in lab notebook 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128111fc",
   "metadata": {},
   "source": [
    "## Task 1: Cluster frequencies and concordances are complementary\n",
    "\n",
    "As shown below, there are over 12000 concordance lines for \"I\". Although we can learn things from the concordance, it is not straightforward to systematically work through 600 pages of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'i'\n",
    "conc.concordance(query, context_length = 8, order='1L1R2R', page_current = 1, page_size = 20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405d4ab",
   "metadata": {},
   "source": [
    "N-gram clusters provide a way to quickly identify the most frequent token sequences that include \"I\".\n",
    "\n",
    "The cell below shows how to get a table of cluster frequencies for trigrams starting with \"I\". The Conc \n",
    "library uses the `ngrams` method to return n-gram clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i\"\n",
    "conc.ngrams(query, ngram_length = 3, ngram_token_position = 'LEFT').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798aa3b0",
   "metadata": {},
   "source": [
    "Change the `ngram_token_position` to `RIGHT` to see trigrams ending with \"I\". You can change the length of the n-grams \n",
    "using the `ngram_length` parameter to explore longer or shorter sequences.\n",
    "\n",
    "You will notice that this provides a quick way to identify, quantify and summarise token sequences that are \n",
    "present in a concordance.  Copy and paste one of the n-grams from the table above into the concordance \n",
    "query variable above to examine examples in the corpus (e.g. \"i had to\"). \n",
    "\n",
    "The table of n-gram clusters and the concordance are complementary, especially when dealing with lots of data. \n",
    "The frequency information provides a way to identify the most common sequences. The concordance provides \n",
    "a way to examine the rich language data and explore what authors are doing through their language use.  \n",
    "\n",
    "Note: As with the previous Conc tables you have encountered, you can change the number of rows displayed by \n",
    "setting a `page_size` parameter. You can page through the results using `page_current`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8dc3a4",
   "metadata": {},
   "source": [
    "Create a markdown cell and note your responses to the following question. \n",
    "\n",
    "* The n-gram clusters for \"I\" represent instances where the author is the \"main character\" in their earthquake story. Note down some features of the authors' use of \"I\" in these accounts you have identified through your analysis of n-gram clusters and the complementary concordances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec640a0",
   "metadata": {},
   "source": [
    "## Task 2: Back to \"Time\" and connecting n-gram clusters and collocation analysis\n",
    "\n",
    "In the previous notebook, you explored use of \"time\" using collocation analysis. Compare the results of the collocation table you created for \"time\" with n-gram clusters for \"time\". \n",
    "\n",
    "Notice that this provides a way to identify which of the collocates appear in relatively fixed phrases, and those that do not. \n",
    "\n",
    "Create a markdown cell and note your observations about how these settings change the results you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770c1ff",
   "metadata": {},
   "source": [
    "## Task 3: Examining n-gram frequencies\n",
    "\n",
    "A table of frequent n-grams is a useful exploratory tool when starting out analysing a corpus. To get a frequency list of n-grams, use Conc's `ngram_frequencies` method. The table below shows the most frequent 4-grams in the Quake Stories corpus.\n",
    "\n",
    "Experiment with changing the `ngram_length` parameter.\n",
    "\n",
    "With the n-gram length set to 4, examine the first few n-grams using concordances. \n",
    "\n",
    "Make some notes in a markdown cell about the following questions:\n",
    "\n",
    "- Is there a 4-gram that sticks out from the first five results? \n",
    "- What steps are required to validate what is causing this pattern? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db071e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.ngram_frequencies(ngram_length=4, page_current=1).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b9fee",
   "metadata": {},
   "source": [
    "## Task 4: An exploratory task to wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d5b99",
   "metadata": {},
   "source": [
    "Create a new code cell and copy the code to create an N-gram frequency table. Try different settings for cluster size, but this time look further down the frequency list by setting higher `page_current` numbers (e.g. for 4-grams try two or three pages between 15 and 50). The results further down the list of n-grams are often just as interesting as the very frequent clusters. Take some time to explore less frequent n-grams using a concordance and note down your observations in a markdown cell:\n",
    "\n",
    "- Identify two interesting n-grams and what you found out by concordancing these patterns.\n",
    "\n",
    "It is very likely you have explored patterns and made observations that are different to others in the class. Discuss your findings with someone else in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c46943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
