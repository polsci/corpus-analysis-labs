{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c9f13b",
   "metadata": {},
   "source": [
    "# DIGI405 Lab 1.1: Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7a8da",
   "metadata": {},
   "source": [
    "## Lab 1 Introduction\n",
    "\n",
    "Each week in DIGI405 labs you will work through a worksheet or Jupyter Notebook with a\n",
    "series of tasks. Be sure to take notes as you work through the lab. Talk to your tutors as you\n",
    "work through these tasks – they are there to help and prompt you, as well as discuss your\n",
    "observations. I would also encourage you to discuss your work with your classmates (whether\n",
    "in-person or on Zoom) and learn together. Pace yourself so you can complete as much of the\n",
    "material as possible during lab time.\n",
    "\n",
    "Today’s class activities will focus on obtaining frequency and dispersion information from\n",
    "texts, and to analyse tokens or phrases in context, using concordance analysis. We will take\n",
    "some time to explore the “Introduce yourself” corpus and learn about your classmates.\n",
    "\n",
    "To get started, run the first few cells to load the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conc.corpus import Corpus\n",
    "from conc.conc import Conc\n",
    "from conc.core import get_stop_words\n",
    "from conc.corpora import get_nltk_corpus_sources\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ecf8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = f'/srv/source-data/' # path to the source data from which the corpus will be created\n",
    "save_path = f'/srv/corpora/' # path to the directory where corpora are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus().load(f'{save_path}introduce-yourself.corpus') # load the corpus\n",
    "reference_corpus = Corpus().load(f'{save_path}brown.corpus') # load the Brown corpus for comparisons later in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running the code on your own machine, adjust the corpus_source_path below to point to the zip with the source files\n",
    "# uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# corpus_source_path = f'{source_path}introduce_yourself_corpus.zip'\n",
    "# try:\n",
    "#     corpus = Corpus().load(f'{save_path}introduce-yourself.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     corpus = Corpus(name = 'Introduce Yourself', description = 'The \"Introduce Yourself\" corpus is based on responses to the Introduce Yourself task from previous semesters of DIGI405. Names have been replaced with NAMEREMOVED.').build_from_files(corpus_source_path, f'{save_path}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are running the code on your own machine, adjust the corpus_source_path below to point to the zip with the source files\n",
    "# # uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# get_nltk_corpus_sources(source_path) # download brown, reuters and gutenberg corpora via NLTK\n",
    "# try:\n",
    "#     reference_corpus = Corpus().load(f'{save_path}brown.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     reference_corpus = Corpus(name = 'Brown', description = '').build_from_csv(f'{source_path}brown.csv.gz', f'{save_path}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.summary() # overview of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = Conc(corpus) #initialize Conc to report on your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_reference = Conc(reference_corpus) #initialize Conc to report on the reference corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words(save_path = save_path) # load spaCy stop words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef7ca0",
   "metadata": {},
   "source": [
    "If you want to examine the contents of the source data for the Introduce Yourself corpus, uncomment the following cell and run it. This will copy it into your current working directory. You can download it to your local computer by right clicking on the file in file viewer and clicking 'Download'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751926c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = '/srv/source-data/introduce_yourself_corpus.zip' # path to the file containing the Introduce Yourself texts\n",
    "# destination = os.path.join(os.getcwd(), os.path.basename(source_file))\n",
    "# shutil.copy(source_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552dff12",
   "metadata": {},
   "source": [
    "## Frequency \n",
    "\n",
    "Frequency data about tokens in a corpus is important information for corpus analysis.  Frequency tables can be used to identify words that are frequent and infrequent, to compare the frequency of related words in a corpus, and compare frequency between corpora. \n",
    "\n",
    "This lab will introduce you to frequency analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca438c6",
   "metadata": {},
   "source": [
    "## Task 1: What tokens are frequent?\n",
    "\n",
    "Here is an example frequency table based on the 'Introduce yourself' corpus. Look first at the information at the bottom of the table. Notice that there is information on the kinds of tokens being displayed (i.e. words and punctuation tokens), as well as quantitative information on the total number of tokens and the number of unique tokens. This is page 1 of many pages of results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.frequencies(exclude_punctuation = False, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55695e57",
   "metadata": {},
   "source": [
    "Now focus on the information in the table. You will notice there is a rank number, the token frequency and a normalized frequency. The normalized frequencies are normalized by 1000 tokens. You can adjust the `normalize_by` parameter as needed (e.g. to 10000 or 1000000).  \n",
    "\n",
    "Create a markdown cell below and make notes on your observations. \n",
    "\n",
    "What kinds of tokens are most frequent in the corpus? \n",
    "\n",
    "You can change the `page_current` number to see other results pages. Try this now. \n",
    "\n",
    "What kinds of tokens appear more as you view more results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e17b1",
   "metadata": {},
   "source": [
    "## Task 2: What is interesting and what is informative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e53f34",
   "metadata": {},
   "source": [
    "The code below adds the `exclude_punctuation` parameter to filter out punctuation tokens. Our interest for this analysis is word tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705301b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.frequencies(exclude_punctuation = True, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3baa9",
   "metadata": {},
   "source": [
    "When looking at a frequency table, we are often in a rush to find interesting tokens for analysis. One way to do this is to discard information on the most frequent tokens using a stop word list. A stop word list typically contains the most frequent function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.frequencies(exclude_punctuation = True, page_current = 1, normalize_by=1000, exclude_tokens = stop_words).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee85f1",
   "metadata": {},
   "source": [
    "The frequency table now shows more content words related to the topics of the corpus. Before looking closely at this, we should understand the information we've lost. The cell below prints the stop words. Think about your introduction text, which is an example of the texts that make up the corpus. If our aim is to understand linguistic patterns related to this corpus, are there any words that you think might be informative that we are discarding?\n",
    "\n",
    "Create a markdown cell and make a note of any tokens in the stop words that you think might be informative (and why). \n",
    "\n",
    "Have you come up with some tokens? You can examine these more closely using the concordance notebook later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55070b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f1565",
   "metadata": {},
   "source": [
    "### Important: don't discard information ...\n",
    "\n",
    "Often people new to language analysis approach it as a recipe, a series of steps that they follow the same way every time regardless of their data or research aims.  \n",
    "\n",
    "**Do not automatically remove stop words!** While stop word removal is relevant for specific tasks (e.g. applying machine learning techniques to model topics in a corpus), you may be discarding important information. \n",
    "\n",
    "Stop words matter! To illustrate this, consider the title of this section \"Important: don't discard information\". If we remove stop words, this becomes \"Important: discard information\". A completely different message.\n",
    "\n",
    "**Do not use stop word lists to discard tokens unless:**\n",
    "\n",
    "1. you have done some exploratory analysis initially to understand the data and the kinds of tokens that are present; and,   \n",
    "2. you are aware of alternative ways to identify tokens used more or less frequently than other language samples (e.g. like statistical measures you will learn about in the next module); and,  \n",
    "3. you have weighed up the alternatives and you have a good reason to make a choice that makes sense for your research question and the data you are working with; and,  \n",
    "3. you record your thinking about this choice for yourself and others.  \n",
    "\n",
    "This is a more general point to keep in mind when analysing data. We often make choices during analysis. Does your choice make sense for your analysis? Are you able to explain and justify your choices to other researchers? You should be able to say \"Yes!\" to both of these questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94ee7e",
   "metadata": {},
   "source": [
    "## Task 3: Corpus analysis is comparative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f30434",
   "metadata": {},
   "source": [
    "Now go back to the frequency table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8be71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.frequencies(exclude_punctuation = True, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcb119",
   "metadata": {},
   "source": [
    "One problem with making sense of the frequency table is that it lacks context. We probably don't have a good sense of which tokens are typically frequent or not. We almost certainly don't have a good sense of what frequency data means without comparison.  \n",
    "\n",
    "Let's compare the Introduce yourself frequency table with the frequency table of a general sample of English. Our *reference corpus* is the Brown Corpus, which includes approximately 1 million words of American English text from the 1960s. We might question whether the kind of English and the dates are relevant, these are good questions to ask. It is useful for our comparison here, because it contains a wide range of text types. First, here is a summary of the corpus. You can compare this to the summary of the Introduce yourself corpus at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_corpus.summary() # overview of the reference corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352bcb2",
   "metadata": {},
   "source": [
    "Here is the frequency table for the Brown Corpus. You can change the `page_current` number to see other results pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_reference.frequencies(exclude_punctuation = True, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d35490",
   "metadata": {},
   "source": [
    "Compare the two frequency tables. You will notice the most frequent tokens are different for each corpus. While comparing ranks gives you some information about the relative frequency of tokens within the corpus, the normalized frequency is very helpful quantitative information about how frequencies in the two corpora compare. With the normalized frequency you can use quantitative information to express differences between the corpora. For example:\n",
    "\n",
    "The function word \"my\" appears appears 21.9 times more frequently per 1000 tokens in the Introduce yourself corpus than in the Brown corpus.  \n",
    "\n",
    "Note: this statement is based on the following information:  \n",
    "* Normalized frequency of \"my\" in Introduce yourself corpus: 27.2  \n",
    "* Normalized frequency of \"my\" in Brown corpus: 1.24  \n",
    "* Ratio of normalized frequencies: 27.2 / 1.24 = 21.9  \n",
    "\n",
    "Create a markdown cell and make notes on key differences. Be specific and use the quantitative information in the table to support your observations. To aide you with this you can use a feature of the reporting library to restrict the results to specific tokens. For example, to restrict the results shown to the pronouns \"I\" and \"my\" you would do this:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6317bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_tokens = ['i', 'my'] # change or add tokens as needed - use lowercase\n",
    "conc.frequencies(exclude_punctuation = True, restrict_tokens = restrict_tokens, page_current = 1, normalize_by=1000).display()\n",
    "conc_reference.frequencies(exclude_punctuation = True, restrict_tokens = restrict_tokens, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589968e0",
   "metadata": {},
   "source": [
    "Note: while we can do comparisons with frequency tables, in the next module you will learn about statistics used to compare corpora.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786d8ac",
   "metadata": {},
   "source": [
    "## Task 4: Introducing dispersion in frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb535128",
   "metadata": {},
   "source": [
    "So far we have examined frequent tokens without considering if they appears in many or few texts in the corpus. Function words are generally common across texts, but there will be tokens that are frequent but only appear in very few texts. To examine this, we can add the `show_document_frequency` parameter to the frequency table. This shows the number of documents that mention the token at least once.  \n",
    "\n",
    "In the table below we are restricting the results to a list of words related to text analysis. You can add words to this list if you like. This functionality is helpful to compare counts for related words in a corpus.  \n",
    "\n",
    "Words that do not appear in the corpus are not included in the frequency table.  \n",
    "\n",
    "Notice the related tokens \"languages\" and \"language\". We can express differences between the use of these tokens in terms of their document frequencies. The token \"languages\" appears in 11 of the 96 document (11%), while \"language\" appears in 18 of 96 documents (19%).  \n",
    "\n",
    "Note: We are dealing with a very small corpus in this instance that was derived from a specific prompt. With such a small corpus, the choices made by single authors can be  significant. If we are interested in making general claims about features of texts in the Introduce yourself corpus, we may treat frequent tokens that appear in single (or very few) documents as outliers or exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_tokens = ['language', 'languages', 'linguist', 'linguistics', 'linguistic', 'text', 'corpus', 'corpora', 'texts', 'speech', 'written', 'classify', 'model', 'classification', 'classifier', 'classifiers', 'data'] # change or add tokens as needed - use lowercase\n",
    "conc.frequencies(exclude_punctuation = True, restrict_tokens = restrict_tokens, show_document_frequency = True, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f2d15",
   "metadata": {},
   "source": [
    "If we are working with a larger corpus specific documents are unlikely to account for high frequency tokens, however the distribution of tokens is still relevant. For example, 'data' is the most frequent of the tokens in our list in both the Introduce yourself and Brown corpora (see below). However, \"data\" is mentioned 167 times (16.84 times per 1000 word tokens) in the Introduce yourself corpus and in almost all documents (92 out of 96 texts). In the Brown corpus the token 'data' is mentioned 173 times (0.18 times per 1000 word tokens) and only appears in 10% of documents. This is a very different pattern. The use of \"data\" in the Brown corpus would still be interesting to analyse, but it is helpful to understand that we would be analyzing a token that is relatively common in a subset of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_reference.frequencies(exclude_punctuation = True, restrict_tokens = restrict_tokens, show_document_frequency= True, page_current = 1, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f60697",
   "metadata": {},
   "source": [
    "Can you find any other words that appear multiple times, but only appear in one or two documents?  Make a note of these in a markdown cell for further analysis later in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ace890",
   "metadata": {},
   "source": [
    "## Task 5: Don't make assumptions about meaning from frequency tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8e8c4",
   "metadata": {},
   "source": [
    "Below is a frequency table of mentions of education-related words in the Introduce yourself corpus. One problem to be aware of when analyzing frequency tables and other high-level quantitative views of a corpus is that we are losing the context of the tokens. We need to be careful to avoid making assumptions about what specific tokens mean or why they are frequent or infrequent. \n",
    "\n",
    "In this example, given we are interested in education-related terms we might assume that \"learning\" is frequent because students are talking about their learning or their attitude to learning. However, this would be missing a key pattern in the data. Around one quarter of mentions of the token \"learning\" in this corpus relate to \"machine learning\" or \"deep learning\". Perhaps you've guessed this, especially if you wrote about \"machine learning\", but think of someone analyzing this corpus without that knowledge. With the frequency table alone we cannot make strong claims about the use of \"learning\". \n",
    "\n",
    "This example illustrates why we should not make assumptions about meaning from frequency tables. To understand how tokens are being used, we need information on that token's textual context. This is something we pick up on in the next notebook. \n",
    "\n",
    "Review the tables above. Identify two or three tokens and write a bad assumption about each. You can test them using the concordance notebook later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa893a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_tokens = ['school', 'teaching', 'learning', 'knowledge', 'student', 'training', 'teacher', 'lecturer', 'academic', 'university', 'research', 'skill', 'study', 'educational', 'skills', 'students', 'classroom', 'class']\n",
    "conc.frequencies(exclude_punctuation = True, restrict_tokens = restrict_tokens, page_size = 10, show_document_frequency = True, normalize_by=1000).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397eee6",
   "metadata": {},
   "source": [
    "## Task 6: Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8d898",
   "metadata": {},
   "source": [
    "Rather than answering questions about a corpus, frequency analysis often raises questions. Review the tasks you worked through above. Make notes on the following questions in a markdown cell:  \n",
    "\n",
    "* Do you understand why discarding stop words can be a problem?  \n",
    "* Do you understand the importance of avoiding making assumptions based on frequency tables alone? Did you come up with some bad assumptions to test in the concordance notebook?  \n",
    "* Are there any other tokens you would like to explore further based on your analysis above? Note these down or create hypotheses about the use of these tokens in the corpus.  \n",
    "\n",
    "Now it is time to look at the next notebook, which will introduce you to concordance analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
