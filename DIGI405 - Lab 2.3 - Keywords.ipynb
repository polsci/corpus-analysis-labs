{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2732d05",
   "metadata": {},
   "source": [
    "# DIGI405 Lab 2.3: Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac533c9",
   "metadata": {},
   "source": [
    "## Lab 2 Introduction\n",
    "\n",
    "In the lab notebooks for module 2 we introduce collocation analysis, analysis of clusters and n-grams, and\n",
    "keyword analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eab061",
   "metadata": {},
   "source": [
    "### A note about the Quake Stories v2 corpus\n",
    "\n",
    "This notebook works with the Quake Stories v2 (QSv2) corpus. This data comes from\n",
    "http://www.quakestories.govt.nz/, and consists of crowd-sourced accounts of earthquake experiences\n",
    "following the 2011 Canterbury earthquakes. This corpus contains 487 self-reported stories of\n",
    "earthquake experiences from 2011 to 2019. It is licensed under Creative Commons BY-NC-SA. Please\n",
    "be aware that some stories may relate to people who were killed or injured in the earthquakes. Please\n",
    "treat the material with respect.\n",
    "\n",
    "Remember, you can read about the filename format in the README file included in the corpus zip\n",
    "file. This provides a way for you to view the original web page that each text was scraped from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b87518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conc.corpus import Corpus\n",
    "from conc.listcorpus import ListCorpus\n",
    "from conc.conc import Conc\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = f'/srv/source-data/' # path to the source data from which the corpus will be created\n",
    "save_path = f'/srv/corpora/' # path to the directory where corpora are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus().load(f'{save_path}quake-stories-v2.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a85752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are running the code on your own machine, unzip the source files - adjust the corpus_source_path below to point to the directory with the source files\n",
    "# # uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# corpus_source_path = f'{source_path}quake-stories-v2.zip'\n",
    "# try:\n",
    "#     corpus = Corpus().load(f'{save_path}quake-stories-v2.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     corpus = Corpus(name = 'Quake Stories v2', description = 'This is a corpus based on stories from the http://www.quakestories.govt.nz/ website established by ManatÅ« Taonga / Ministry for Culture and Heritage in 2011. QuakeStories was a place for the public to share stories of these and subsequent New Zealand earthquakes. The site was licensed under Creative Commons BY-NC-SA (https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en). This data-set, as a re-representation of the stories, is also released under BY-NC-SA. Please be aware that some stories may relate to people who were killed or injured in the Canterbury earthquakes. Treat the material with respect. ').build_from_files(corpus_source_path, f'{save_path}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38939ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.summary() # overview of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = Conc(corpus) #initialize Conc reporting with the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1dabd",
   "metadata": {},
   "source": [
    "If you want to examine the contents of the source data for the Quake Stories v2 corpus, uncomment the following cell and run it. This will copy it into your current working directory. You can download it to your local computer by right clicking on the file in file viewer and clicking 'Download'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = '/srv/source-data/quake-stories-v2.zip' # path to the file \n",
    "# destination = os.path.join(os.getcwd(), os.path.basename(source_file))\n",
    "# shutil.copy(source_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913f3a3",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "This lab notebook will introduce you to keyword analysis. \n",
    "\n",
    "In the frequency analysis lab notebook (1.1) you learned how to remove stop words from a frequency table. However, we also learned that frequent function words are revealing about the contents of a corpus and, therefore, removing stop words might be a poor and arbitrary research practice that obscures meaningful and revealing patterns in the data. Rather than arbitrarily removing frequent words we find uninteresting, the statistical measures used in keyword analysis provide a way identify frequency patterns that are over-represented in one corpus (the target corpus) compared to another (the reference corpus). These are referred to as *keywords*.\n",
    "\n",
    "Keyword analysis is a helpful way into a corpus, as it provides a way to identify what is distinctive about the corpus we are analysing. To analyse keywords we combine techniques introduced through the lab notebooks to date. Keyword tables provide information on frequency patterns for specific tokens that are distinctive. We can make use of collocation analysis and concordancing to understand the word choices represented by keywords. \n",
    "\n",
    "This notebook will continue analysis of the Quake Stories v2 corpus introduced in lab notebook 2.1 and 2.2. In the second part of this notebook you will be analyse a corpus of political speeches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128111fc",
   "metadata": {},
   "source": [
    "## Section 1: Comparing a specialist corpus with a general reference corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa89ebb",
   "metadata": {},
   "source": [
    "In Section 1 we compare the Quake Stories v2 with a general reference corpus. This kind of comparison is useful to identify what is distinctive about the target corpus and how it differs from language use more generally. In this instance, we are comparing texts authored in New Zealand in the 2010s with a general reference corpus of British English written and spoken language use from the 1990s, namely the British National Corpus (BNC). Although we could think through problems with this comparison, the BNC is often used as a general reference corpus due to its size (100m words) and high quality. \n",
    "\n",
    "The following cells load a lightweight representation of frequency information from the BNC for use as a reference corpus. If you had the full BNC, you could use that instead, but for keyword analysis, we only need the frequency of tokens in the reference corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_corpus = ListCorpus().load(f'{save_path}bnc.listcorpus') # loading a Conc list corpus as a reference corpus\n",
    "# # if you had the full BNC, you could use this instead:\n",
    "# reference_corpus = Corpus().load(f'{save_path}bnc.corpus') # loadding a Conc corpus as a reference corpus\n",
    "# # See the Conc documentation site for information on creating a list corpus from the BNC https://geoffford.nz/conc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b24c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_corpus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a92c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.set_reference_corpus(reference_corpus) # setting the reference corpus for keyword analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757da492",
   "metadata": {},
   "source": [
    "The table below shows the top 20 keywords in the Quake Stories v2 corpus compared to the BNC reference corpus. Notice that many of the keywords are terms related to the major topic of the corpus, i.e. the earthquakes in Canterbury, New Zealand in 2010 and 2011.  \n",
    "\n",
    "In this instance the table is ordered by the Log Likelihood ratio, which is statistical significance measure. This ordering ranks keywords based on the statistical evidence for over-use in Quake Stories v2 compared to the BNC.  \n",
    "\n",
    "Notice, the table includes a number of columns:\n",
    "\n",
    "- `Frequency` and `Frequency Reference` are the raw frequencies of the keyword in the target and reference corpora. These cannot be directly compared without taking into account the size of the target and reference corpora. \n",
    "- `Normalized Frequency` and `Normalized Frequency Reference` are normalized frequencies in each corpus. These can be directly compared.\n",
    "- `Relative Risk` is an effect size measure that can be directly related to the normalized frequencies (it is the ratio of these two values). One way to think about this is that it shows how much more a keyword occurs in the target compared to the reference corpus. The more similar the values of the normalized frequency in the target and reference, the closer the relative risk will be to 1. Values less than 1 indicate under-use in the target corpus when compared to the reference corpus. Values greater than 1 indicate over-use in the target. The greater the value the larger the difference in normalized frequencies.\n",
    "- `Log Ratio` is an effect size measure derived from relative risk that is intuitive to interpret. For more information, read this short article: [Log Ratio - an informal introduction](https://cass.lancs.ac.uk/log-ratio-an-informal-introduction/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.keywords(page_current = 1, page_size = 20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7074b6",
   "metadata": {},
   "source": [
    "Create code cells and markdown cells as needed to complete each task below and answer the related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d20ac0",
   "metadata": {},
   "source": [
    "### Task 1.1: A statistical basis to identify salient function words\n",
    "\n",
    "When analysing a keyword table we are often interested in identifying related words. An obvious group of related keywords in the table above are âfunction wordsâ. Here we have a statistical basis to make sense of very frequent function words. We can be more specific than this though. What type of words are these, mainly and why do you think they are over-represented in the Quake Stories corpus when compared with the BNC reference corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a61d3",
   "metadata": {},
   "source": [
    "### Task 1.2: Investigating keywords using collocates and concordance\n",
    "\n",
    "To make sense of keywords and how they are being used, we need to make use of collocation tables, concordances, and tables of n-gram clusters. You can copy code from other lab notebooks to do this now. Analyse some of the keywords in the table above:  \n",
    "\n",
    "* âbuildingâ and âbuildingsâ - some of the instances relate to specific building names - what is the easiest way to find these?\n",
    "* âshakingâ \n",
    "* âliquefactionâ \n",
    "* compare âaftershockâ and âaftershocksâ \n",
    "* pick another word (perhaps one you donât understand or one you are interested in)\n",
    "\n",
    "Make notes on the patterns you observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770c1ff",
   "metadata": {},
   "source": [
    "## Section 2: Comparing sub-corpora or related specialised corpora\n",
    "\n",
    "Another way the Keywords List can be used is to compare sub-corpora, or related specialised corpora. For instance, Paul Baker compared two sides of the parliamentary debate over fox hunting in Britain by creating a sub-corpus for the pro-hunting speakers and one for the anti-hunting speakers. This can be a good strategy for identifying differences between the way groups or sides of a debate are representing an issue.\n",
    "\n",
    "For this part of the lab we will work with the Beehive (2014-2019) corpus. This consists of speeches by New Zealand government ministers scraped from https://www.beehive.govt.nz/search?f%5B0%5D=content_type_facet%3Aspeech. In the zip file you download you have two folders: one is a subcorpus for the last National-led government (during the period 2014-2017) and the other is a subcorpus for the Labour-NZ First Coalition (from 2017 to 2019). National is a centre-right party, which has historically had strong support from the business and farming sectors. Labour is a centre-left party, which has historically had strong support from urban workers, the civil service and trade unions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a429f",
   "metadata": {},
   "source": [
    "### Caution: Making claims about political discourse\n",
    "\n",
    "Avoid making claims like this: \"Tokens A, B, and C are over-represented in the Labour corpus. This means Labour cares more about issue X and is more concerned with action on Y than National\". \n",
    "\n",
    "This is a claim about beliefs and intentions, but we are studying language use. Believe it or not, the things politicians say may not be a good indication of what they believe, what they will do, and why they are doing it. Avoid making claims that are not supported by the data or other research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758683e",
   "metadata": {},
   "source": [
    "The following cells load the two sub-corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24718a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "labour_corpus = Corpus().load(f'{save_path}labour-nz-first-coalition.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15dfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are running the code on your own machine, unzip the source files - adjust the corpus_source_path below to point to the directory with the source files\n",
    "# # uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# corpus_source_path = f'{source_path}beehive-speeches-2014-2019/2017-2020 Labour-New Zealand First Coalition'\n",
    "# try:\n",
    "#     labour_corpus = Corpus().load(f'{save_path}labour-nz-first-coalition.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     labour_corpus = Corpus(name = 'Labour-NZ First Coalition', description = 'Labour-NZ First Coalition (2017-2019) subcorpus, part of the Beehive (2014-2019) corpus compiled by Dr Geoff Ford. The corpus consists of speeches by New Zealand government ministers scraped from https://www.beehive.govt.nz/.').build_from_files(corpus_source_path, f'{save_path}', standardize_word_token_punctuation_characters = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labour_corpus.summary() # overview of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_corpus = Corpus().load(f'{save_path}national-led.corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e47fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are running the code on your own machine, unzip the source files - adjust the corpus_source_path below to point to the directory with the source files\n",
    "# # uncomment the remaining lines of this cell to create the corpus from the source files (or load if it already exists)\n",
    "# corpus_source_path = f'{source_path}beehive-speeches-2014-2019/2014-2017 National-led'\n",
    "# try:\n",
    "#     national_corpus = Corpus().load(f'{save_path}national-led.corpus')\n",
    "# except FileNotFoundError:\n",
    "#     national_corpus = Corpus(name = 'National-led', description = 'National-led government (during the period 2014-2017) subcorpus, part of the Beehive (2014-2019) corpus compiled by Dr Geoff Ford. The corpus consists of speeches by New Zealand government ministers scraped from https://www.beehive.govt.nz/.').build_from_files(corpus_source_path, f'{save_path}', standardize_word_token_punctuation_characters = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "national_corpus.summary() # overview of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dc41d",
   "metadata": {},
   "source": [
    "If you want to examine the contents of the source data for the Beehive corpus, uncomment the following cell and run it. This will copy it into your current working directory. You can download it to your local computer by right clicking on the file in file viewer and clicking 'Download'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc364b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_file = '/srv/source-data/beehive-speeches-2014-2019.zip' # path to the file \n",
    "# destination = os.path.join(os.getcwd(), os.path.basename(source_file))\n",
    "# shutil.copy(source_file, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315a5dc",
   "metadata": {},
   "source": [
    "Below we set Labour as the target corpus and National as the reference corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = Conc(labour_corpus) #initialize Conc reporting with the target corpus\n",
    "conc.set_reference_corpus(national_corpus) # set the reference corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784ac94",
   "metadata": {},
   "source": [
    "Now, we're going to create a keywords table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.keywords(page_current = 1, page_size = 20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e03d4a",
   "metadata": {},
   "source": [
    "Add code or markdown cells as needed to complete the tasks below and answer the related questions. You can copy code from other lab notebooks as needed to look more closely at specific keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd50cb1",
   "metadata": {},
   "source": [
    "### Task 2.1: Grouping keywords\n",
    "\n",
    "Examine the top 20 keywords and try to group related or connected keywords. Make sure you examine how the keywords are used (e.g. concordance them), not just assume connections between them. What does this tell us about us about some of the key ideas of the Labour government when compared with the National government?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c2cf3",
   "metadata": {},
   "source": [
    "### Task 2.2: Negative keywords\n",
    "\n",
    "By default, Conc keyword tables exclude negative keywords, which are words that are under-used in the target corpus when compared with the reference corpus. The `exclude_negative_keywords` parameter can be set to `False` to include negative keywords. \n",
    "\n",
    "You can identify negative keywords by finding words with a negative log ratio value. Negative keywords have a relative risk score less than 1. \n",
    "\n",
    "What do the negative keywords tell us about the change in focus of the Labour government compared to the National government that preceded it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93561d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.keywords(page_current = 1, page_size = 20, exclude_negative_keywords=False).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d09b64",
   "metadata": {},
   "source": [
    "### Task 2.3: Effect size and filtering by statistical significance and frequency\n",
    "\n",
    "So far, we've been sorting by statistical significance, but this does not express the size of the differences in frequency between the two corpora. We can use Conc's `order` parameter to order by `log_ratio`. This tends to rank words that are very rare (or absent) from the reference corpus higher than words that are frequent in both. \n",
    "\n",
    "There are two key ways to filter results:\n",
    "\n",
    "* Use a `statistical_significance_cut`: Choose a p value to remove keywords based on statistical significance (you can be aggressive with this). In addition, we can impose a stricter test that takes into account the number of tests being conducted using the [Bonferonni correction](https://en.wikipedia.org/wiki/Bonferroni_correction).  \n",
    "* Use frequency filtering: Conc supports the `min_frequency` and `max_frequency_reference` parameters to filter keywords based on their frequencies in the target and reference corpora, as well as `min_document_frequency` and `min_document_frequency_reference` to filter keywords based on the number of documents they appear in.\n",
    "\n",
    "The `show_document_frequency` parameter has been added so that we can look at the dispersion of keywords across documents. \n",
    "\n",
    "Experiment with these parameters now. \n",
    "\n",
    "Spend some time analsing the keywords in the table. \n",
    "\n",
    "Questions:\n",
    "\n",
    "- How does this keywords list, ordered by the effect size measure log ratio, compare with the first table? \n",
    "- What does this new ordering of keywords add to your analysis from 2.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db071e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.keywords(page_current=1, page_size = 20, order = 'log_ratio', statistical_significance_cut=0.0001, apply_bonferroni=True, min_frequency_reference = 10, show_document_frequency = True).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b9fee",
   "metadata": {},
   "source": [
    "## Task 4: Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d5b99",
   "metadata": {},
   "source": [
    "Are you done?\n",
    "\n",
    "If you want to explore these techniques further, here are some suggestions: \n",
    "\n",
    "1.\tThink about the kinds of comparisons that would be possible using the Quake Stories data. Look at the filename scheme discussed in the README and think about other ways the texts could be grouped and compared.  \n",
    "2.\tSpend some time exploring the âIntroduce yourselfâ corpus using some of the techniques you have learned this week, including:  \n",
    "    - Examining frequency lists of n-grams â notice that these include mentions of common entities and frequent phrases people are using to narrate their âintroductionâ.\n",
    "    - Comparing the corpus with the BNC reference word list;\n",
    "    - Examining the collocates of frequent content words (e.g. âdataâ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c46943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
